{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vmb37-ua/curso_tensorflow2025/blob/master/02-Overfit%20and%20underfit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9F5XgDUwgrT"
      },
      "source": [
        "# *Overfit* y *underfit*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Tjb56SwgrV"
      },
      "source": [
        "## Objetivos\n",
        "- Comprender qué son y por qué se producen fenómenos como el sobreajuste u *overfitting* y el subajuste o *underfitting*.\n",
        "- Conocer diferentes técnicas que favorecen que nuestro modelo generalice mejor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tMPlmwyPwgrV",
        "outputId": "fa6f7d4f-86b4-431c-84a1-816dccfc5df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcGDV2IwgrW"
      },
      "source": [
        "## Base de datos de IMDB con reseñas de películas\n",
        "\n",
        "La base de datos de IMDB se utiliza principalmente en el área de procesamiento de lenguaje natural ya que contiene 50000 reseñas de usuarios sobre películas. El objetivo es entrenar un modelo que sea capaz de determinar si la reseña es positiva o negativa. Por tanto, se trata de un problema de clasificación.\n",
        "\n",
        "La base de datos IMDB es una de las más populares en el ámbito del *deep learning* y está disponible en [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews).\n",
        "\n",
        "Aunque en ingeniería química los datos que se manejan suelen ser de tipo numérico, esta base de datos textual es muy ilustrativa para profundizar en conceptos como el *overfitting* y el *underfitting*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5YkCVQjwwgrX",
        "outputId": "316bf040-1d6d-4a0c-e3f2-48a0e14f6ec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (val_data, val_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    # Create an all-zero matrix of shape (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.  # set specific indices of results[i] to 1s\n",
        "    return results\n",
        "\n",
        "# Our vectorized training data\n",
        "X_train = vectorize_sequences(train_data)\n",
        "# Our vectorized test data\n",
        "X_valid = vectorize_sequences(val_data)\n",
        "# Our vectorized labels\n",
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_valid = np.asarray(val_labels).astype('float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFBUUuGYwgrX"
      },
      "source": [
        "## Demostrar *overfiting* y *underfiting*\n",
        "En esta sección vamos a entrenar diferentes configuraciones o arquitecturas de redes neuronales siguiendo el mismo procedimiento de entrenamiento para poder comparar las diferencias entre ellas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L7J5O7awgrX"
      },
      "source": [
        "### Definir los modelos\n",
        "\n",
        "La capacidad de un modelo se refiere al tamaño y la complejidad de los patrones que es capaz de aprender. En el caso de las redes neuronales, depende del número de neuronas que tenga y de cómo estén conectadas entre sí. Por tanto, si parece que una red no se ajusta bien a los datos, se debe aumentar su capacidad.\n",
        "\n",
        "Hay dos maneras de aumentar la capacidad de una red neuronal:\n",
        "- Haciendo la red más ancha (añadiendo más neuronas a las capas existentes).\n",
        "- Haciendo la red más profunda (añadiendo más capas).\n",
        "\n",
        "A las redes más anchas les resulta más fácil aprender relaciones  lineales, mientras que las redes más profundas prefieren las no lineales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnsjTgQfwgrY"
      },
      "source": [
        "Primero, construye un modelo lineal que sirva como referencia. En los problemas de clasificación, sí que se aplica una función de activación de tipo sigmoide en la *output layer*, pues ayuda a restringir los valores de salida entre 0 (valoración negativa) y 1 (valoración positiva).\n",
        "\n",
        "<img src=https://miro.medium.com/v2/resize:fit:640/format:webp/1*jSCPkJo0ZpBRA5H3JqFhQg.png width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_zZq4TdiwgrY"
      },
      "outputs": [],
      "source": [
        "linear = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=[X_train.shape[1]]),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XBM4O6dwgrY"
      },
      "source": [
        "El siguiente modelo será una red neuronal con una única capa oculta de 5 neuronas y función de activación ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dtvlfdFXwgrY"
      },
      "outputs": [],
      "source": [
        "small = keras.Sequential(\n",
        "    [\n",
        "        layers.Input(shape=[X_train.shape[1]]),\n",
        "        layers.Dense(5, activation='relu'),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNUvMeG_wgrZ"
      },
      "source": [
        "Por comparación, crea una red neuronal que sea más \"ancha\" que la anterior. Por ejemplo, incrementa el número de neuronas a 32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjv711FtwgrZ"
      },
      "outputs": [],
      "source": [
        "wider = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AB792akIwgrZ"
      },
      "source": [
        "Finalmente, crea una red que sea más profuda, doblando el número de capas ocultas con respecto al modelo `small`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM0skfahwgrZ"
      },
      "outputs": [],
      "source": [
        "deeper = # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKuXeYNwgrZ"
      },
      "source": [
        "### Establecer un método de entrenamiento común\n",
        "\n",
        "Como se va a emplear la misma configuración para los comandos `compile` y `fit` y, además, se va a aplicar a los diferentes modelos, resulta conveniente agrupar todo este proceso en una función que llamaremos `compile_and_fit`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO7bfTfBwgrZ"
      },
      "outputs": [],
      "source": [
        "N_TRAIN = len(y_train)\n",
        "BATCH_SIZE = 512\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOxxRrH3wgra"
      },
      "outputs": [],
      "source": [
        "def compile_and_fit(model, callback , max_epochs=EPOCHS):\n",
        "\n",
        "  model.compile(optimizer='rmsprop',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  history = model.fit(\n",
        "                      _____________________ , # Completa con las inputs de entrenamiento\n",
        "                      _____________________ , # Completa con las outputs de entrenamiento\n",
        "                      validation_data= _____________________ , # Completa con los datos de validación en forma de tupla (X,y)\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      epochs=EPOCHS,\n",
        "                      callbacks=callback, # put your callbacks in a list\n",
        "                      verbose=0,  # turn off training log\n",
        "                      )\n",
        "\n",
        "  return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaKXoCKqwgra"
      },
      "source": [
        "### Comparar el proceso de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdGzIA9vwgra"
      },
      "outputs": [],
      "source": [
        "size_histories = {}\n",
        "size_histories['linear'] = compile_and_fit(linear, [])\n",
        "size_histories['small'] = compile_and_fit(small, [])\n",
        "size_histories['wider'] = compile_and_fit(wider, [])\n",
        "size_histories['deeper'] = compile_and_fit(deeper, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeJAAMjywgra"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation loss:\")\n",
        "for model_name, history in size_histories.items():\n",
        "    print(\"    - \"+model_name+\": \"+str(min(history.history['val_loss'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eTCmdF4wgra"
      },
      "outputs": [],
      "source": [
        "def plot_history(model_name, history, ylim=None):\n",
        "    history_df = pd.DataFrame(history.history)\n",
        "    history_df.loc[:, ['loss', 'val_loss']].plot()\n",
        "    plt.title(model_name+' model')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    if ylim is not None:\n",
        "        plt.ylim(ylim)\n",
        "\n",
        "plot_history('linear', size_histories['linear'],)\n",
        "plot_history('small', size_histories['small'],)\n",
        "plot_history('wider', size_histories['wider'],)\n",
        "plot_history('deeper', size_histories['deeper'], )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2p8O85gLwgrb"
      },
      "source": [
        "## Técnicas para prevenir el sobreajuste"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6QW6gQtwgrb"
      },
      "source": [
        "#### *Early stopping*\n",
        "\n",
        "Hemos visto que los modelos tienden a aprender inclus el ruido que hay en los datos tratando de ajustarse lo máximo posible a los datos de entrenamiento. Cuando esto ocurre, la *loss function* medida en el set de validación puede comienza a aumentar.\n",
        "\n",
        "Para evitarlo, podemos detener el entrenamiento cuando parezca que la pérdida de validación ya no disminuye. Interrumpir el entrenamiento de esta forma se denomina *early stopping* y en TensorFlow se implementa por medio de un *callback*.\n",
        "\n",
        "Los *callback* son simplemente una función que se ejecutan cada cierto tiempo mientras la red se entrena. El *early stopping* lo hace después de cada época. Tensorflow cuenta con una [variedad de callbacks útiles predefinidos](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks), pero también pueden definirse de forma personalizada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3inYZAYFwgrb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor = ____________________ , # Magnitud que se va a \"vigilar\"\n",
        "    min_delta= ______________________ , # Mínimo cambio para considerar como mejora\n",
        "    patience= _____________________ , # Épocas que esperamos para parar cuando se deja de mejorar\n",
        "    restore_best_weights=True,\n",
        ")\n",
        "\n",
        "model = # TODO (elige el modelo small)\n",
        "\n",
        "history = compile_and_fit(model, [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynGsmXfTwgrb"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation loss:\")\n",
        "print(\"    - Early stopping model: \"+str(min(history.history['val_loss'])))\n",
        "\n",
        "plot_history('Early stopping', history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WupH0dZmwgrb"
      },
      "source": [
        "#### Regularización\n",
        "Una forma común de mitigar el sobreajuste es forzando a que los pesos tomen valores pequeños. Esto se llama **regularización** y se hace añadiendo a la *loss function* de la red un coste asociado a tener pesos grandes. Este coste puede ser de dos tipos:\n",
        "\n",
        "* [Regularización L1](https://developers.google.com/machine-learning/glossary/#L1_regularization), donde el coste añadido es proporcional al valor absoluto de los coeficientes de los pesos (es decir, a lo que se denomina la «norma L1» de los pesos).\n",
        "\n",
        "* [Regularización L2](https://developers.google.com/machine-learning/glossary/#L2_regularization), en la que el coste añadido es proporcional al cuadrado del valor de los coeficientes de las ponderaciones (es decir, a lo que se denomina la «norma L2» al cuadrado de los pesos).\n",
        "\n",
        "La regularización L1 fomenta que los pesos tomen el valor de 0 exactamente mientras que la regularización L2 penalizará los parámetros de pesos sin hacerlos totalmente 0, ya que la penalización va a cero para pesos pequeños. Es por esto que la L2 es más común en el contexto de redes neuronales, ya que favorece el aprendizaje al reducir el riesgo de *vanishing gradients*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVV9tdNbwgrc"
      },
      "outputs": [],
      "source": [
        "l2_model = # TODO\n",
        "\n",
        "history = compile_and_fit(l2_model, [early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koeB-K3owgrc"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation loss:\")\n",
        "print(\"    - L2-Reg model: \"+str(min(history.history['val_loss'])))\n",
        "\n",
        "plot_history('l2_reg',history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xa_PrT4wgrc"
      },
      "source": [
        "#### Dropout\n",
        "El *dropout* es una de las técnicas de regularización de redes neuronales más eficaces y utilizadas, que consiste en «descartar» aleatoriamente una serie de características de salida de la capa durante el entrenamiento. Por ejemplo, una capa determinada habría devuelto normalmente un vector `[0,2, 0,5, 1,3, 0,8, 1,1]` para una muestra de entrada determinada durante el entrenamiento; después de aplicar el dropout, este vector tendrá unas cuantas entradas cero distribuidas al azar, por ejemplo `[0, 0,5, 1,3, 0, 1,1]`.\n",
        "\n",
        "El *dropout rate* es la fracción de las características que se eliminan; normalmente se fija entre 0,2 y 0,5.\n",
        "\n",
        "En TensorFlow, puedes introducir el abandono en una red a través de la capa `tf.keras.layers.Dropout`, que se aplica a la salida de la capa justo antes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zDEg4cGwgrc"
      },
      "outputs": [],
      "source": [
        "dropout_model = # TODO\n",
        "\n",
        "history = compile_and_fit(dropout_model, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtiAOU60wgrd"
      },
      "outputs": [],
      "source": [
        "print(\"Minimum validation loss:\")\n",
        "print(\"    - Dropout model: \"+str(min(history.history['val_loss'])))\n",
        "\n",
        "plot_history('dropout',history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XObzccEywgrd"
      },
      "source": [
        "## Conlcusiones\n",
        "Para recapitular, éstas son las formas más comunes de evitar el sobreajuste en las redes neuronales:\n",
        "\n",
        "* Obtener más datos de entrenamiento.\n",
        "* Reducir la capacidad de la red.\n",
        "* Añadir regularización de pesos.\n",
        "* Añadir abandono."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env_curso_tensorflow",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}